{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f283d3-5721-4f64-91c8-bf905d1cd403",
   "metadata": {},
   "source": [
    "# Data preparation \n",
    "\n",
    "Prepare Stanford Cars dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c6ba5-a8e4-4e88-be42-de4da6aca4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import torch \n",
    "import torchvision\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da6857-1b08-403b-bd29-90dfb032bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a477f-6189-4587-8e45-4883d5922378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader for Stanford Cars dataset \n",
    "import torch.utils.data as data \n",
    "import os \n",
    "from typing import Literal\n",
    "from PIL import Image\n",
    "\n",
    "class CarsDataset(data.Dataset): \n",
    "    def __init__(self, opt: Literal['test', 'train'], transform = None): \n",
    "        super(CarsDataset, self).__init__()\n",
    "        \n",
    "        path = kagglehub.dataset_download(\"jessicali9530/stanford-cars-dataset\")\n",
    "\n",
    "        self.test_path = f'{path}/cars_test/cars_test/'\n",
    "        self.train_path = f'{path}/cars_train/cars_train/'\n",
    "\n",
    "        # set if the dataset is for train or test \n",
    "        self.opt = opt \n",
    "        self.transform = transform \n",
    "\n",
    "        if opt == 'test':\n",
    "            self.path = self.test_path \n",
    "        else:\n",
    "            self.path = self.train_path \n",
    "        self.cnt = None \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        idx = idx + 1\n",
    "        total = self.__len__()\n",
    "\n",
    "        # handle out of index \n",
    "        if idx <= 0 or idx > total:\n",
    "            raise Exception(\"Invalid idx access to dataset\")\n",
    "            return None \n",
    "\n",
    "        # get image of name with idx\n",
    "        file_name = f'{idx:05}.jpg'\n",
    "        image = Image.open(os.path.join(self.path, file_name))\n",
    "\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image \n",
    "    def __len__(self): \n",
    "        # count the numbers of image in the test or train directory\n",
    "        if self.cnt != None:\n",
    "            return self.cnt \n",
    "        \n",
    "        cnt = 0\n",
    "        for file in os.listdir(self.path): \n",
    "            if os.path.isfile(os.path.join(self.train_path, file)): \n",
    "                cnt += 1\n",
    "        self.cnt = cnt \n",
    "        return cnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cd7ff-1725-4086-82c6-506ee93a3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training Configuration \n",
    "BATCH_SIZE = 128\n",
    "INIT_LR = 1e-3\n",
    "IMG_SIZE = 64\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e43e87-e2cb-4842-84f7-84b5e436a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dataloader \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(), # scales the data into [0, 1]\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1) # scale between [-1, 1]\n",
    "])\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda t: (t+1) / 2), \n",
    "    transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "    transforms.Lambda(lambda t: t * 255.), \n",
    "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)), \n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "# define datasets \n",
    "train_data = CarsDataset('train', transform=data_transform)\n",
    "test_data = CarsDataset('test', transform=data_transform)\n",
    "\n",
    "# concat dataset because no need for test data in Diffusion model \n",
    "train_data = train_data + test_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, generator= torch.Generator(device=device), drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle=False, generator= torch.Generator(device=device), drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019312a-aaae-4581-8074-908e333d3c4a",
   "metadata": {},
   "source": [
    "# Implement the Forward process of Diffusion Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aeea43-4d9e-4e5d-807e-2e2469a52671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "# generate beta_t for each timesteps \n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):  \n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "# returns the specific index t \n",
    "def get_index_from_list(vals, t, x_shape): \n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.to(device))\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# take image x_0 as input, and return noisy version of it \n",
    "def forward_diffusion_sample(x_0, t):\n",
    "    \n",
    "    noise = torch.randn_like(x_0) \n",
    "\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "    \n",
    "    # return mean + variance * noise, noise \n",
    "    \n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "    \n",
    "\n",
    "# Define beta schedule \n",
    "T = 100 \n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# pre-calculate different terms for closed form \n",
    "alphas = 1. - betas \n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas_cumprod)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88ed0c-16f1-49a1-ad50-2f4a0ebeec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# define function show_tensor_image \n",
    "def show_tensor_image(image): \n",
    "    if len(image.shape) == 4: \n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transform(image))\n",
    "\n",
    "\n",
    "# Iterate noising process over one image \n",
    "image = next(iter(train_dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T / num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize): \n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images + 1, (idx // stepsize) + 1) \n",
    "    image, noise = forward_diffusion_sample(image, t) \n",
    "    show_tensor_image(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f13ebc-4df3-4e6d-9c76-a4b1bdb0bb1c",
   "metadata": {},
   "source": [
    "# Implement Backward Process using U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21faf543-5bd9-4070-9894-493f9fe9b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import math \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, kernel_size=3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e8e79-101f-4cfd-a26d-5f858a30ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module): \n",
    "    def __init__(self, dim): \n",
    "        super().__init__() \n",
    "        self.dim = dim \n",
    "\n",
    "    def forward(self, time): \n",
    "        device = time.device \n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1) \n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6763b-d581-4e6b-8c69-de3b5c51563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUnet(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        image_channels = 3 \n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32 \n",
    "\n",
    "        # Time embedding \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim), \n",
    "            nn.Linear(time_emb_dim, time_emb_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Initial projection \n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsample \n",
    "        self.down_samples = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels) - 1)])\n",
    "\n",
    "        # Upsample \n",
    "        self.up_samples = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels) - 1)])\n",
    "\n",
    "        # match the final output dimension \n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1) \n",
    "\n",
    "    def forward(self, x, timestep): \n",
    "        h = x.to(device)\n",
    "        t = timestep.to(device)\n",
    "        # embed time \n",
    "        t = self.time_mlp(timestep)\n",
    "\n",
    "        # push image to initial projection(conv0)\n",
    "        h = self.conv0(h)\n",
    "\n",
    "        # apply unet \n",
    "        residual_inputs = [] \n",
    "\n",
    "        for down in self.down_samples: \n",
    "            h = down(h, t)\n",
    "            residual_inputs.append(h)\n",
    "\n",
    "        for up in self.up_samples: \n",
    "            residual_h = residual_inputs.pop()\n",
    "\n",
    "            h = torch.cat((h, residual_h), dim=1)\n",
    "            h = up(h, t)\n",
    "\n",
    "        return self.output(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768c326-5b2a-4858-b2e0-091757421a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Unet model \n",
    "model = SimpleUnet() \n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaefb9d-bcae-4b35-a807-f87c5aa9b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function for training \n",
    "mseloss = nn.MSELoss()\n",
    "def get_loss(model, x_0s, t): \n",
    "    x_noisy, noise = forward_diffusion_sample(x_0s, t) \n",
    "    noise_pred = model(x_noisy, t) \n",
    "    return mseloss(noise, noise_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7372bc5-e0c7-4e19-832e-3f8359a36dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one image a.k.a generate one image using Diffusion model \n",
    "\n",
    "# get image from specific timestep from (timestep+1) image\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t): \n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    # print(f'model_mean: {model_mean}')\n",
    "\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t == 0: \n",
    "        return model_mean \n",
    "    else: \n",
    "        noise = torch.rand_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image(): \n",
    "    img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    num_images=10 \n",
    "    stepsize=int(T/num_images)\n",
    "\n",
    "    for i in range(0, T)[::-1]: \n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t) \n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "\n",
    "        if i % stepsize == 0: \n",
    "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d9823-e48c-4dac-9f57-60f8d6e1f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5dc69-d378-4185-a330-fd8b395d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \n",
    "from torch.optim import Adam \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), INIT_LR)\n",
    "\n",
    "\n",
    "# go through one epoch \n",
    "def train_epoch(epoch_num):\n",
    "    \n",
    "    loss_sum = 0. \n",
    "    last_loss = 0.\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        loss = get_loss(model, batch, t)\n",
    "        loss.backward() \n",
    "        loss_sum += loss.item()\n",
    "        last_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0: \n",
    "            print(f'Epoch {epoch_num}, Step {step} | Last Loss: {last_loss}')\n",
    "            sample_plot_image()\n",
    "\n",
    "    avg_loss = loss_sum / (step + 1)\n",
    "    print(f'@@ Epoch {epoch_num} | Average Loss: {avg_loss} | Last Loss: {last_loss}')\n",
    "\n",
    "    # show sample plot image \n",
    "    sample_plot_image()\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a4df6-bd2f-4820-b4e9-a6796d6a3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training happens here\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# create models directory if not exists\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "best_avg_loss = 100_000_000\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    avg_loss = train_epoch(epoch)\n",
    "    \n",
    "    if best_avg_loss > avg_loss: \n",
    "            best_avg_loss = avg_loss\n",
    "            model_path = f'models/model_{timestamp}_{epoch}'\n",
    "            torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
